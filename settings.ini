# if you just have no equal sign on the line, than the parameter will be None
# create 'user_settings.ini' and use it to override settings for testing
[model]
int seed = 13

preprocessed_train = output_train
# None param
preprocessed_test

preprocessed_val = output_val

int weight_save_epoch_period = 50


start_weights_path

int batch_size = 48
int samples_per_epoch = 2400
int val_samples = 1200
int num_epoch = 100
model_output_dir = model_output

#--------------------------------------------------------------------
# lr experiments
#--------------------------------------------------------------------
[GRU_1_05]
optimizer = nadam
float learn_rate = 1.00e-05
float beta_1 = 0.9
float beta_2 = 0.999
float epsilon = 1e-08
float schedule_decay= 0.004
pretrained_word_vectors_file

[GRU_1_04]
optimizer = nadam
float learn_rate = 1.00e-04
float beta_1 = 0.9
float beta_2 = 0.999
float epsilon = 1e-08
float schedule_decay= 0.004
pretrained_word_vectors_file

[GRU_5_04]
optimizer = nadam
float learn_rate = 5.00e-04
float beta_1 = 0.9
float beta_2 = 0.999
float epsilon = 1e-08
float schedule_decay= 0.004
pretrained_word_vectors_file

[GRU_1_03]
optimizer = nadam
float learn_rate = 1.00e-03
float beta_1 = 0.9
float beta_2 = 0.999
float epsilon = 1e-08
float schedule_decay= 0.004
pretrained_word_vectors_file

[GRU_1_03_glove]
optimizer = nadam
float learn_rate = 1.00e-03
float beta_1 = 0.9
float beta_2 = 0.999
float epsilon = 1e-08
float schedule_decay= 0.004
pretrained_word_vectors_file = output_train/initial_word_embeddings_matrix.npy

[GRU_2_03]
optimizer = nadam
float learn_rate = 2.00e-03
float beta_1 = 0.9
float beta_2 = 0.999
float epsilon = 1e-08
float schedule_decay= 0.004
pretrained_word_vectors_file

[GRU_5_03]
optimizer = nadam
float learn_rate = 5.00e-03
float beta_1 = 0.9
float beta_2 = 0.999
float epsilon = 1e-08
float schedule_decay= 0.004
pretrained_word_vectors_file

[GRU_adam_1_03]
optimizer = adam
float learn_rate = 1.00e-03
float beta_1 = 0.9
float beta_2 = 0.999
float epsilon = 1e-08
float decay= 0.0
pretrained_word_vectors_file

[GRU_stacked]
num_epoch = 200
pretrained_word_vectors_file

[GRU_12_words_sentence]
optimizer = nadam
float learn_rate = 1.00e-03
float beta_1 = 0.9
float beta_2 = 0.999
float epsilon = 1e-08
float schedule_decay= 0.004
pretrained_word_vectors_file

[GRU_20_words_sentence]
optimizer = nadam
float learn_rate = 1.00e-03
float beta_1 = 0.9
float beta_2 = 0.999
float epsilon = 1e-08
float schedule_decay= 0.004
pretrained_word_vectors_file
#--------------------------------------------------------------------

[default_model]
optimizer = nadam
float learn_rate = 1.00e-03
float beta_1 = 0.9
float beta_2 = 0.999
float epsilon = 1e-08
float schedule_decay= 0.004

pretrained_word_vectors_file = output_train/initial_word_embeddings_matrix.npy

[lstm_nadam_model]
optimizer = nadam
float learn_rate = 0.0005
float beta_1 = 0.9
float beta_2 = 0.999
float epsilon = 1e-08
float schedule_decay= 0.004

pretrained_word_vectors_file


[preprocess]
int seed = 17
int max_sentence_length = 16
int min_token_instances = 5
int max_images = 0
int image_work_threads = 8

#pretrained_word_embeddings = data/glove.6B.200d.txt
pretrained_word_embeddings

[tests]
id_to_word_file = output_train/id_to_word.json
test_source = test_images
preprocessed_text_file = output_train/preprocessed_text.h5
preprocessed_images_file = output_train/preprocessed_images.h5
preprocessed_embeddings_dir = output_train
