# if you just have no equal sign on the line, than the parameter will be None
# create 'user_settings.ini' and use it to override settings for testing
[model]
int seed = 42

preprocessed_train = output_train
# None param
preprocessed_test

preprocessed_val = output_val

int weight_save_epoch_period = 10


start_weights_path

int batch_size = 50
int samples_per_epoch = 1000
int val_samples = 500
int num_epoch = 100
model_output_dir = model_output

#--------------------------------------------------------------------
# lr experiments
#--------------------------------------------------------------------
[vanilla_1_05]
optimizer = nadam
float learn_rate = 1.00e-05
float beta_1 = 0.9
float beta_2 = 0.999
float epsilon = 1e-08
float schedule_decay= 0.004
pretrained_word_vectors_file

[vanilla_1_04]
optimizer = nadam
float learn_rate = 1.00e-04
float beta_1 = 0.9
float beta_2 = 0.999
float epsilon = 1e-08
float schedule_decay= 0.004
pretrained_word_vectors_file

[vanilla_5_04]
optimizer = nadam
float learn_rate = 5.00e-04
float beta_1 = 0.9
float beta_2 = 0.999
float epsilon = 1e-08
float schedule_decay= 0.004
pretrained_word_vectors_file

[vanilla_1_03]
optimizer = nadam
float learn_rate = 1.00e-03
float beta_1 = 0.9
float beta_2 = 0.999
float epsilon = 1e-08
float schedule_decay= 0.004
pretrained_word_vectors_file

[vanilla_2_03]
optimizer = nadam
float learn_rate = 2.00e-03
float beta_1 = 0.9
float beta_2 = 0.999
float epsilon = 1e-08
float schedule_decay= 0.004
pretrained_word_vectors_file

[vanilla_5_03]
optimizer = nadam
float learn_rate = 5.00e-03
float beta_1 = 0.9
float beta_2 = 0.999
float epsilon = 1e-08
float schedule_decay= 0.004
pretrained_word_vectors_file
#--------------------------------------------------------------------


[default_model]
optimizer = adam
float learn_rate = 0.0002
float beta_1 = 0.9
float beta_2 = 0.999
float epsilon = 1e-08
float decay= 0.0

pretrained_word_vectors_file = output_train/initial_word_embeddings_matrix.npy

[lstm_nadam_model]
optimizer = nadam
float learn_rate = 0.0005
float beta_1 = 0.9
float beta_2 = 0.999
float epsilon = 1e-08
float schedule_decay= 0.004

pretrained_word_vectors_file


[preprocess]
int seed = 17
int max_sentence_length = 16
int min_token_instances = 5
int max_images = 0
int image_work_threads = 8

pretrained_word_embeddings = data/glove.6B.200d.txt

[tests]
id_to_word_file = output_train/id_to_word.json
test_source = test_images
preprocessed_text_file = output_train/preprocessed_text.h5
preprocessed_images_file = output_train/preprocessed_images.h5
preprocessed_embeddings_dir = output_train
